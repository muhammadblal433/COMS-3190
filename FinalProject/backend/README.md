üß† EasyCal Project Setup
Welcome to EasyCal - an AI-powered nutrition and recipe app!
This guide walks you through setting up the project locally for the first time.

## Prerequisites
Before you begin, make sure you have these installed on your system:

‚úÖ Node.js (v16 or higher)
‚úÖ Python 3.9+
‚úÖ pip (Python package manager)
‚úÖ MongoDB (local or remote instance)
‚úÖ Ollama ‚Üí download from the website (https://ollama.com/download) for AI model support

## Installation
1Ô∏è‚É£ Run this once inside your /backend folder:
Backend dependencies:
Run this inside the /backend folder:
npm install
pip install fastapi uvicorn requests

2Ô∏è‚É£ Frontend dependencies:
Run this inside the /frontend folder:
npm install

## How to Run Everything
You'll need 4 terminal windows open to run everything:

1Ô∏è‚É£ Start the Frontend (React + Vite)
cd frontend
npm run dev

This runs the React app at: http://localhost:5173/

2Ô∏è‚É£ Start the Node.js Express Backend API
cd backend
npm run dev

This runs the Express server at: http://localhost:3000/
This server connects to MongoDB

3Ô∏è‚É£ Start the AI FastAPI Server (Python)
cd backend
uvicorn aiServer:app --host 127.0.0.1 --port 5005

This runs the AI middleware at: http://127.0.0.1:5005/

4Ô∏è‚É£ Start the Ollama AI Model
First time only, pull the model, AND MAKE SURE TO DOWNLOAD OLLAMA FROM WEBSITE (https://ollama.com/download) FIRST, as mentioned in our Prerequisites.
ollama pull tinyllama:1.1b-chat

Then, every time you start:
ollama run tinyllama:1.1b-chat

This runs the Ollama LLM server at: http://localhost:11434/

‚úÖ Quick Summary
frontend ‚Üí React app (localhost:5173)
backend ‚Üí Node.js API (localhost:3000)
aiServer (backend) ‚Üí Python FastAPI (localhost:5005)
ollama (backend) ‚Üí LLM model server (localhost:11434)

